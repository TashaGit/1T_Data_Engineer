# Импортируем библиотеки
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.utils.dates import days_ago
from airflow.models import Variable
from airflow.hooks.base import BaseHook
from datetime import datetime, timedelta
import requests, csv, json
import psycopg2
import pandas as pd

# Список акций для парсинга данных.
securities = ['GAZP', 'SBER', 'GMKN', 'VTBR', 'YNDX']

# Параметры по умолчанию
default_args = {
    "owner": "marina_z",
    # 'start_date': days_ago(1),
    'retry_delay': timedelta(minutes=5),
}

# Создаем DAG ручного запуска (инициализирующий режим).
one_time_start_dag = DAG(dag_id='one_time_start_dag',
                         tags=['marina_z'],
                         start_date=datetime(2023, 8, 29),
                         schedule_interval=None,
                         default_args=default_args
)


# Загружаем переменные из JSON файла
with open('/opt/airflow/dags/config.json', 'r') as config_file:
    my_variables = json.load(config_file)

dag_variables = Variable.set("shares_variable", my_variables, serialize_json=True)

# Получение значений из переменной окружения.
dag_variables = Variable.get('shares_variable', deserialize_json=True)


# Получение объекта Connection с помощью метода BaseHook.get_connection
def get_conn_credentials(conn_id) -> BaseHook.get_connection:
    """
    Function returns dictionary with connection credentials

    :param conn_id: str with airflow connection id
    :return: Connection
    """
    conn = BaseHook.get_connection(conn_id)
    return conn


# Получаем данные соединения с базой данных из переменных DAG
pg_conn = get_conn_credentials(dag_variables.get('connection_name'))
# Извлекаем параметры соединения с базой данных
pg_hostname, pg_port, pg_username, pg_pass, pg_db = pg_conn.host, pg_conn.port, pg_conn.login, pg_conn.password, pg_conn.schema


# Создаем подключение к базе данных PostgreSQL с помощью полученных параметров
conn = psycopg2.connect(
    host=pg_hostname,
    port=pg_port,
    user=pg_username,
    password=pg_pass,
    database=pg_db,
    options=dag_variables.get('options')
)


# Извлекаем переменные из variables
folder_path = dag_variables.get('path')
csv_file_path = dag_variables.get('csv_file_path')
column_names = dag_variables.get('column_names')

# Функция с параметром "интервал" = 60, которая скачивает исторические данные торгов на Москвоской бирже по курсу акций
# и сохраняет в csv - файл (Инициализирующий режим)
def create_csv_files():
    for security in securities:
        interval = 60
        start_date = "2000-01-01"
        end_date = (datetime.now() - timedelta(days=2)).strftime("%Y-%m-%d")
        url = f"https://iss.moex.com/iss/engines/stock/markets/shares/boards/TQBR/securities/{security}/candles.json?interval={interval}&from={start_date}&till={end_date}"
        response = requests.get(url)
        result = json.loads(response.text)
        resp_date = result["candles"]["data"]
        columns = result["candles"]["columns"]
        data_shares = pd.DataFrame(resp_date, columns=columns)
        data_shares = data_shares.assign(ticker=security, company=dag_variables.get(security))
        len_df = len(resp_date)
        last_received_date = data_shares.iloc[-1]["begin"]

        while len_df != 0:
            start_date = (pd.to_datetime(last_received_date) + pd.Timedelta(minutes=interval)).strftime("%Y-%m-%d %H:%M:%S")
            url = f"https://iss.moex.com/iss/engines/stock/markets/shares/boards/TQBR/securities/{security}/candles.json?interval={interval}&from={start_date}&till={end_date}"
            response = requests.get(url)
            result = json.loads(response.text)
            resp_date = result["candles"]["data"]
            len_df = len(resp_date)
            if len_df == 0:
                break
            data_next_page = pd.DataFrame(resp_date, columns=columns)
            data_next_page = data_next_page.assign(ticker=security, company=dag_variables.get(security))
            data_shares = pd.concat([data_shares, data_next_page], ignore_index=True) # Объединяем данные
            last_received_date = data_shares.iloc[-1]["begin"] # записываем последнее значение даты

        data_shares.to_csv(f'./raw_data/raw_{security}.csv', index=False)
        print(f'Файл raw_{security}.csv сохранен.')

# Функция создания таблиц и загрузки данных в таблицы (Инициализирующий режим)
def create_raw_tables():
    for security in securities:
        cur = conn.cursor()
        table_name = f'raw_{security}'
        # Заранее определенные столбцы с типами данных
        columns = """
            id INTEGER PRIMARY KEY NOT NULL GENERATED BY DEFAULT AS IDENTITY ( INCREMENT 1 ),
            open_val DECIMAL,
            close_val DECIMAL,
            high DECIMAL,
            low DECIMAL,
            value DECIMAL,
            volume BIGINT,
            start_time TIMESTAMP,
            end_time TIMESTAMP,
            ticker VARCHAR, 
            company VARCHAR
        """

        sql_query = f"""
            DROP TABLE IF EXISTS {table_name};
            CREATE TABLE {table_name}
            (
                {columns}
            )
        """
        try:
            cur.execute(sql_query)
            conn.commit()
            print("Таблица создана!")

        except Exception as e:
            print(f"Ошибка создания таблицы {table_name}:", str(e))
            conn.rollback()


# Функция загрузки данных в таблицы (Инициализирующий режим)
def load_data():
    for security in securities:
        cur = conn.cursor()
        table_name = f'raw_{security}'
        psql_raw_path = f'{dag_variables.get("psql_raw_path")}raw_{security}.csv'
        with open(f'{csv_file_path}raw_{security}.csv', encoding='UTF-8') as f:
            reader = csv.reader(f)
            next(reader)  # Skip header
            print(f'Файл raw_{security}.csv прочитан')
        copy_query = f"""
                        COPY {table_name} ({column_names})
                        FROM '{psql_raw_path}'
                        WITH (FORMAT CSV,
                              DELIMITER ',',
                              HEADER,
                              ENCODING 'UTF-8');
                    """
        try:
            cur.execute(copy_query)
            conn.commit()
            print(f"Данные в таблицу загружены из csv: '{psql_raw_path}'!")
        except Exception as e:
            print(f"Ошибка заполнения таблицы {table_name}:", str(e))
            conn.rollback()

def execute_ddl():
    cur = conn.cursor()
    with open('/opt/airflow/sql_scripts/ddl/ddl.sql', 'r') as f:
        script = f.read()
    cur.execute(script)
    conn.commit()


def execute_dml():
    cur = conn.cursor()
    with open('./sql_scripts/dml/dml.sql', 'r') as f:
        script = f.read()
    cur.execute(script)
    conn.commit()


# Таск создания файлов (Инициализирующий режим)
create_csv_task = PythonOperator(
    task_id='create_csv_files',
    python_callable=create_csv_files,
    dag=one_time_start_dag
)

# Таск создания таблиц на слое raw (Инициализирующий режим)
create_raw_tables_task = PythonOperator(
    task_id='create_raw_tables',
    python_callable=create_raw_tables,
    dag=one_time_start_dag
)

# Таск загрузки данных в БД из csv фалов (Инициализирующий режим)
load_data_task = PythonOperator(
    task_id='load_data',
    python_callable=load_data,
    dag=one_time_start_dag
)

execute_ddl_task = PythonOperator(
	task_id='execute_ddl',
	python_callable=execute_ddl,
	dag=one_time_start_dag
)

execute_dml_task = PythonOperator(
	task_id='execute_dml',
	python_callable=execute_dml,
	dag=one_time_start_dag
)


# Очередность запуска task-ов
create_csv_task >> create_raw_tables_task >> load_data_task >> execute_ddl_task >> execute_dml_task


# Создаем новый DAG для загрузки дельты данных за прошедшие сутки (Инкрементальный режим)
daily_update_dag = DAG(dag_id='daily_update_dag',
                       tags=['daily_update'],
                       start_date=datetime(2023, 9, 9),
                       # schedule_interval=timedelta(days=1),
                       catchup=False,
                       schedule_interval='45 05 * * *',
                       default_args=default_args)


# Функция загрузки дельты данных за прошедшие сутки в таблицы (Инкрементальный режим)
def update_table_from_csv(security):
    table_name = f'raw_{security}'
    psql_raw_path = f'{dag_variables.get("psql_raw_path")}last_day_{security}.csv'
    cur = conn.cursor()

    # Получить список столбцов без колонки id
    copy_query = f"""
                    COPY {table_name} ({column_names})
                    FROM '{psql_raw_path}'
                    WITH (FORMAT CSV,
                          DELIMITER ',',
                          HEADER,
                          ENCODING 'UTF-8');
                """

    try:
        cur.execute(copy_query)
        conn.commit()
        print(f"Дополнительные данные загружены из csv: '{psql_raw_path}' в таблицу {table_name}!")
    except Exception as e:
        print(f"Ошибка дополнения таблицы {table_name}:", str(e))
        conn.rollback()

def create_temp_delta():
    table_name = 'temp_delta'
    cur = conn.cursor()
    # Заранее определенные столбцы с типами данных
    columns = """
        open_val DECIMAL,
        close_val DECIMAL,
        high DECIMAL,
        low DECIMAL,
        value DECIMAL,
        volume BIGINT,
        start_time TIMESTAMP,
        end_time TIMESTAMP,
        ticker VARCHAR, 
        company VARCHAR
    """

    sql_query = f"""
        DROP TABLE IF EXISTS {table_name};
        CREATE TEMPORARY TABLE {table_name}
        (
            {columns}
        )
    """
    try:
        cur.execute(sql_query)
        conn.commit()
        print(f"Таблица {table_name} создана!")

    except Exception as e:
        print(f"Ошибка создания таблицы {table_name}:", str(e))
        conn.rollback()

def update_temp_delta(security):
    table_name = 'temp_delta'
    psql_raw_path = f'{dag_variables.get("psql_raw_path")}last_day_{security}.csv'
    cur = conn.cursor()

    # Получить список столбцов без колонки id
    copy_query = f"""
                    COPY {table_name} ({column_names})
                    FROM '{psql_raw_path}'
                    WITH (FORMAT CSV,
                          DELIMITER ',',
                          HEADER,
                          ENCODING 'UTF-8');
                """
    try:
        cur.execute(copy_query)
        conn.commit()
        print(f"Дополнительные данные загружены из csv: '{psql_raw_path}' в таблицу {table_name}!")
    except Exception as e:
        print(f"Ошибка дополнения таблицы {table_name}:", str(e))
        conn.rollback()


def dml_core_update():
    cur = conn.cursor()
    with open('./sql_scripts/dml/dml_core_update.sql', 'r') as f:
        script = f.read()
    cur.execute(script)
    conn.commit()

# Функция скачивания данных за прошедшие сутки в таблицы (Инкрементальный режим)
# при выполнении условия (проверка последней загруженной даты в таблицу)
def downl_raw_last_day():
    for security in securities:
        interval = 60
        end_date = (datetime.now() - timedelta(days=1)).strftime("%Y-%m-%d")
        with conn.cursor() as cur:
            # Запрос на получение максимального времени начала
            cur.execute(f"SELECT MAX(start_time) FROM raw_{security};")
            last_begin = cur.fetchone()[0]

        start_date = (last_begin.date() + timedelta(days=1)).strftime("%Y-%m-%d")
        if last_begin.date() < datetime.strptime(end_date, "%Y-%m-%d").date():
            url = f"https://iss.moex.com/iss/engines/stock/markets/shares/boards/TQBR/securities/{security}/candles.json?interval={interval}&from={start_date}&till={end_date}"
            response = requests.get(url)
            result = json.loads(response.text)
            resp_date = result["candles"]["data"]
            columns = result["candles"]["columns"]
            data_shares = pd.DataFrame(resp_date, columns=columns)
            data_shares = data_shares.assign(ticker=security, company=dag_variables.get(security))
            if data_shares.shape[0] >= 2:
                data_shares.to_csv(f'./raw_data/last_day_{security}.csv', index=False)
                print(f'Файл last_day_{security}.csv сохранен')
                create_temp_delta()
                update_table_from_csv(security) # Обновление таблицы после сохранения файла
                update_temp_delta(security) # Запись данных во временную таблицу для обновления слоя core
                dml_core_update()
            else:
                print('Данных для записи нет.')


# Функция по созданию слоя Mart - создание витрины данных на основе обновленной информации за прошедшие сутки
def create_data_mart():
    data_mart_table = "data_mart"
    cur = conn.cursor()

    sql_query = f"""
        DROP TABLE IF EXISTS {data_mart_table};
        CREATE TABLE {data_mart_table}
        (
            surrogate_key VARCHAR,
            company VARCHAR,
            date_stock DATE,
            total_share DECIMAL,
            open_val DECIMAL,
            close_val DECIMAL,
            percentage_difference DECIMAL,
            max_volume_interval VARCHAR,
            max_price_interval VARCHAR,
            min_price_interval VARCHAR
        );
    """
    try:
        cur.execute(sql_query)
        conn.commit()
        print(f"Таблица {data_mart_table} создана!")

    except Exception as e:
        print(f"Ошибка создания таблицы {data_mart_table}:", str(e))

    for security in securities:
        core_table_name = 'core_layer_stock_prices'
        cur.execute(f"""
        
        INSERT INTO data_mart (surrogate_key, company, date_stock, total_share, 
        open_val, close_val, percentage_difference, 
        max_volume_interval, max_price_interval, min_price_interval)
        SELECT DISTINCT ON (ticker)
            ticker AS surrogate_key, t.company, date_stock,
            ROUND(SUM(value), 2) AS total_share, 
            (SELECT open_val 
             FROM core_layer_stock_prices
             WHERE date_stock = (SELECT MAX(date_stock) FROM core_layer_stock_prices) 
             AND start_time = (SELECT MIN(start_time) FROM core_layer_stock_prices WHERE date_stock =
             (SELECT MAX(date_stock) FROM core_layer_stock_prices) AND ticker = '{security}')
             AND ticker = '{security}'),
            (SELECT close_val 
             FROM core_layer_stock_prices
             WHERE date_stock = (SELECT MAX(date_stock) FROM core_layer_stock_prices) 
             AND end_time = (SELECT MAX(end_time) FROM core_layer_stock_prices WHERE date_stock =
             (SELECT MAX(date_stock) FROM core_layer_stock_prices) AND ticker = '{security}') AND ticker = '{security}'),
            ROUND((((SELECT open_val 
                      FROM core_layer_stock_prices
                      WHERE date_stock = (SELECT MAX(date_stock) FROM core_layer_stock_prices) 
                      AND start_time = (SELECT MIN(start_time) FROM core_layer_stock_prices WHERE date_stock =
                      (SELECT MAX(date_stock) FROM core_layer_stock_prices) AND ticker = '{security}') AND  ticker = '{security}') /
                    (SELECT close_val 
                      FROM core_layer_stock_prices
                      WHERE date_stock = (SELECT MAX(date_stock) FROM core_layer_stock_prices) 
                      AND end_time = (SELECT MAX(end_time) FROM core_layer_stock_prices WHERE date_stock =
                      (SELECT MAX(date_stock) FROM core_layer_stock_prices) AND ticker = '{security}') AND  ticker = '{security}') * 100 - 100)), 4) AS percentage_difference,
            (SELECT (start_time, end_time)::VARCHAR
             FROM core_layer_stock_prices
             WHERE volume = (SELECT MAX(volume) FROM core_layer_stock_prices WHERE date_stock =
             (SELECT MAX(date_stock) FROM core_layer_stock_prices WHERE ticker = '{security}') AND  ticker = '{security}') LIMIT 1) AS max_volume_interval,        
            (SELECT (start_time, end_time)::VARCHAR
             FROM core_layer_stock_prices
             WHERE high = (SELECT MAX(high) FROM core_layer_stock_prices WHERE date_stock =
             (SELECT MAX(date_stock) FROM core_layer_stock_prices) AND  ticker = '{security}') LIMIT 1) AS max_price_interval,
            (SELECT (start_time, end_time)::VARCHAR
             FROM core_layer_stock_prices
             WHERE high = (SELECT MIN(high) FROM core_layer_stock_prices WHERE date_stock =
             (SELECT MAX(date_stock) FROM core_layer_stock_prices) AND  ticker = '{security}') LIMIT 1) AS min_price_interval
        FROM core_layer_stock_prices
        JOIN tickers t USING (ticker)
        WHERE date_stock = (SELECT MAX(date_stock) FROM core_layer_stock_prices) AND ticker = '{security}'
        AND NOT EXISTS (
            SELECT 1
            FROM data_mart dm
            WHERE dm.surrogate_key = ticker
        )
        GROUP BY core_layer_stock_prices.ticker, t.company, core_layer_stock_prices.date_stock
        ORDER BY ticker;
        """)
        conn.commit()
        print(f"Данные по акции {security} добавлены в таблицу {data_mart_table}.")


# Создание витрины данных (статистика) за всю историю хранения данных
def create_statistic_mart():
    data_mart_table = "statistic_mart"
    core_table_name = 'core_layer_stock_prices'
    cur = conn.cursor()

    sql_query = f"""
        DROP TABLE IF EXISTS {data_mart_table};
        CREATE TABLE {data_mart_table}
        (
            surrogate_key VARCHAR,
            company	VARCHAR,
            first_day DATE,
            last_day DATE,
            max_share DECIMAL,
            date_max_share DATE,
            min_share DECIMAL,
            date_min_share DATE,
            max_volume BIGINT,
            date_max_vol DATE,
            min_volume BIGINT,
            date_min_vol DATE,
            max_price DECIMAL,
            date_max_price DATE,
            min_price DECIMAL,
            date_min_price DATE
        );
    """
    try:
        cur.execute(sql_query)
        conn.commit()
        print(f"Таблица {data_mart_table} создана!")

    except Exception as e:
        print(f"Ошибка создания таблицы {data_mart_table}:", str(e))

    for security in securities:
        core_table_name = 'core_layer_stock_prices'
        cur.execute(f"""
        INSERT INTO {data_mart_table} (surrogate_key, company, first_day,
                                        last_day, max_share, date_max_share,
                                        min_share, date_min_share, max_volume,
                                        date_max_vol, min_volume, date_min_vol,
                                        max_price, date_max_price, min_price, date_min_price)
        SELECT ticker AS surrogate_key, t.company, 
               MIN(date_stock) AS first_day,
               MAX(date_stock) AS last_day,
               ROUND(MAX(value), 2) max_share,
               (SELECT date_stock FROM core_layer_stock_prices WHERE value = (SELECT MAX(value) FROM core_layer_stock_prices WHERE ticker = '{security}') LIMIT 1) AS date_max_share,
               ROUND(MIN(value), 2) min_share,
               (SELECT date_stock FROM core_layer_stock_prices WHERE value = (SELECT MIN(value) FROM core_layer_stock_prices WHERE ticker = '{security}') LIMIT 1) AS date_min_share,     
               MAX(volume) AS max_volume,
               (SELECT date_stock FROM core_layer_stock_prices WHERE volume = (SELECT MAX(volume) FROM core_layer_stock_prices WHERE ticker = '{security}') LIMIT 1) AS date_max_vol, 
               MIN(volume) AS min_volume,
               (SELECT date_stock FROM core_layer_stock_prices WHERE volume = (SELECT MIN(volume) FROM core_layer_stock_prices WHERE ticker = '{security}') LIMIT 1) AS date_min_vol, 
               MAX(high) AS max_price,
               (SELECT date_stock FROM core_layer_stock_prices WHERE high = (SELECT MAX(high) FROM core_layer_stock_prices WHERE ticker = '{security}') LIMIT 1) AS date_max_price,
               MIN(low) AS min_price,
               (SELECT date_stock FROM core_layer_stock_prices WHERE low = (SELECT MIN(low) FROM core_layer_stock_prices WHERE ticker = '{security}') LIMIT 1) AS date_min_price
        FROM core_layer_stock_prices
        JOIN tickers t USING (ticker)
        WHERE ticker = '{security}'
        GROUP BY ticker, company
        HAVING NOT EXISTS (
            SELECT 1
            FROM {data_mart_table}
            WHERE surrogate_key = '{security}'
        )
        ORDER BY ticker;
        """)
        conn.commit()
        print(f"Данные по акции {security} добавлены в таблицу {data_mart_table}.")


# Список Task-ов (Инкрементальный режим) по загрузке в сырой слой данных (raw),
# промежуточный слой (core) и созданию витрин (mart)

downl_raw_last_day_task = PythonOperator(
    task_id='downl_raw_last_day',
    python_callable=downl_raw_last_day,
    dag=daily_update_dag
)

create_data_mart_task = PythonOperator(
    task_id='create_data_mart',
    python_callable=create_data_mart,
    dag=daily_update_dag
)

create_statistic_mart_task = PythonOperator(
    task_id='create_statistic_mart',
    python_callable=create_statistic_mart,
    dag=daily_update_dag
)

# Очередность запуска task-ов
downl_raw_last_day_task >> create_data_mart_task >> create_statistic_mart_task